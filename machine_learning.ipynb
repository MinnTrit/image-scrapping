{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Table, MetaData, select \n",
    "hostname = 'localhost'\n",
    "username = 'root'\n",
    "password = 'Bautroixanh12345'\n",
    "database_name = 'web_scrapping'\n",
    "connection_url = f'mysql://{username}:{password}@{hostname}/{database_name}'\n",
    "engine = create_engine(url=connection_url)\n",
    "table_name1 = 'colors_backup'\n",
    "table_name2 = 'color_hunt'\n",
    "table_name3 = 'color_drop'\n",
    "table_name4 = 'unsplash'\n",
    "metadata = MetaData()\n",
    "table1 = Table(table_name1, metadata, autoload_with=engine)\n",
    "table2 = Table(table_name2, metadata, autoload_with=engine)\n",
    "table3 = Table(table_name3, metadata, autoload_with=engine)\n",
    "table4 = Table(table_name4, metadata, autoload_with=engine)\n",
    "select_statement1 = select(table1)\n",
    "select_statement2 = select(table2)\n",
    "select_statement3 = select(table3)\n",
    "select_statement4 = select(table4)\n",
    "with engine.connect() as connection: \n",
    "    results1 = connection.execute(select_statement1)\n",
    "    rows1 = results1.fetchall()\n",
    "\n",
    "    results2 = connection.execute(select_statement2)\n",
    "    rows2 = results2.fetchall()\n",
    "\n",
    "    results3 = connection.execute(select_statement3)\n",
    "    rows3 = results3.fetchall()\n",
    "\n",
    "    results4 = connection.execute(select_statement4)\n",
    "    rows4 = results4.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df1 = pd.DataFrame(rows1)\n",
    "df2 = pd.DataFrame(rows2)\n",
    "df3 = pd.DataFrame(rows3)\n",
    "df4 = pd.DataFrame(rows4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop(columns='image_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_column(data_frame): \n",
    "    column_list = data_frame.columns.tolist()\n",
    "    column_list = [column.lower() for column in column_list]\n",
    "    if 'score' in column_list: \n",
    "        quantile = [0, 0.25, 0.5, 0.75, 1]\n",
    "        labels = [0, 1, 2, 3]\n",
    "        data_frame['score'] = pd.to_numeric(data_frame['score'])\n",
    "        data_frame.sort_values(by='score', ascending=True, inplace=True)\n",
    "        data_frame.insert(len(column_list), 'rank', pd.qcut(data_frame['score'], q=quantile, labels=labels))\n",
    "    else: \n",
    "        print(f\"Can't conduct ranking for {data_frame}, no column score found\")\n",
    "    return data_frame\n",
    "df2 = ranking_column(df2)\n",
    "df3 = ranking_column(df3) \n",
    "df4 = ranking_column(df4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_learning_df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "machine_learning_df.drop(columns='id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_learning_df['rank'] = pd.to_numeric(machine_learning_df['rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = machine_learning_df['color_code'].tolist()\n",
    "targeted_variable = machine_learning_df['rank'].tolist() \n",
    "vocabulary_size = len(machine_learning_df['color_code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoded_dict = {}\n",
    "label_encoder = LabelEncoder() \n",
    "encoded_color_codes = label_encoder.fit_transform(input_features)\n",
    "for color, encoded_color in zip(input_features, encoded_color_codes): \n",
    "    encoded_dict[color] = encoded_color \n",
    "machine_learning_df['encoded_color_code'] = machine_learning_df['color_code'].map(encoded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is the total of 24812 unique features out of 77080 features\n"
     ]
    }
   ],
   "source": [
    "unique_feature = machine_learning_df['encoded_color_code'].unique().tolist()\n",
    "print(f'There is the total of {len(unique_feature)} unique features out of {len(machine_learning_df)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_rgb(hex_color):\n",
    "    # Remove '#' if present\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    # Convert hex to RGB\n",
    "    rgb = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "    return rgb\n",
    "\n",
    "color_code_list = machine_learning_df['color_code'].tolist()\n",
    "output_list = [] \n",
    "for code in color_code_list:\n",
    "    output = hex_to_rgb(code)\n",
    "    output_list.append(output)\n",
    "\n",
    "red_list = [] \n",
    "green_list = [] \n",
    "blue_list = [] \n",
    "for rgb_code in output_list: \n",
    "    red_channel = rgb_code[0]\n",
    "    green_channel = rgb_code[1]\n",
    "    blue_channel = rgb_code[2]\n",
    "    red_list.append(red_channel)\n",
    "    green_list.append(green_channel)\n",
    "    blue_list.append(blue_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_learning_df['red_channel'] = red_list \n",
    "machine_learning_df['green_channel'] = green_list \n",
    "machine_learning_df['blue_channel'] = blue_list\n",
    "machine_learning_df['red_channel'] = pd.to_numeric(machine_learning_df['red_channel'])\n",
    "machine_learning_df['green_channel'] = pd.to_numeric(machine_learning_df['green_channel'])\n",
    "machine_learning_df['blue_channel'] = pd.to_numeric(machine_learning_df['blue_channel'])\n",
    "features_list = ['encoded_color_code', 'red_channel', 'green_channel', 'blue_channel']\n",
    "input_features = machine_learning_df[features_list].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the input table...\n",
      "The total rows 77080 has been updated\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect \n",
    "inspector = inspect(engine) \n",
    "user = input('Updating the machine learning table?')\n",
    "if user.lower().startswith('y'): \n",
    "    print('Processing the input table...')\n",
    "    while True: \n",
    "        table_name = input('Enter the table name\\n press \"exit\" to escape the function')\n",
    "        if inspector.has_table(table_name): \n",
    "            machine_learning_df.to_sql(table_name, con=engine, index=False, if_exists='replace')\n",
    "            print(f'The total rows {len(machine_learning_df)} has been updated')\n",
    "            break \n",
    "        elif table_name.lower().startswith('e'):\n",
    "            print('Escaping the function')\n",
    "            break   \n",
    "        elif table_name.lower() == 'unsplash': \n",
    "            print('Attempting to access the wrong table')\n",
    "            break \n",
    "        else: \n",
    "            print(f'The table {table_name} was not found')  \n",
    "elif user.lower().startswith('n'): \n",
    "    print('Exiting the current function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-----\n",
      "Looked at 0/61664 samples\n",
      "Looked at 51200/61664 samples\n",
      "Train loss 1.2509 | Test loss 2.6262 | Test acc 0.0\n",
      "Epoch 1\n",
      "-----\n",
      "Looked at 0/61664 samples\n",
      "Looked at 51200/61664 samples\n",
      "Train loss 1.3099 | Test loss 2.6246 | Test acc 0.0\n",
      "Epoch 2\n",
      "-----\n",
      "Looked at 0/61664 samples\n",
      "Looked at 51200/61664 samples\n",
      "Train loss 1.3124 | Test loss 2.6778 | Test acc 0.0\n",
      "Epoch 3\n",
      "-----\n",
      "Looked at 0/61664 samples\n",
      "Looked at 51200/61664 samples\n",
      "Train loss 1.2298 | Test loss 2.6597 | Test acc 0.0\n",
      "Epoch 4\n",
      "-----\n",
      "Looked at 0/61664 samples\n",
      "Looked at 51200/61664 samples\n",
      "Train loss 1.2460 | Test loss 2.6779 | Test acc 0.0\n",
      "Epoch 5\n",
      "-----\n",
      "Looked at 0/61664 samples\n",
      "Looked at 51200/61664 samples\n",
      "Train loss 1.3065 | Test loss 2.6705 | Test acc 0.0\n",
      "Epoch 6\n",
      "-----\n",
      "Looked at 0/61664 samples\n",
      "Looked at 51200/61664 samples\n",
      "Train loss 1.2238 | Test loss 2.6551 | Test acc 0.0\n",
      "Epoch 7\n",
      "-----\n",
      "Looked at 0/61664 samples\n",
      "Looked at 51200/61664 samples\n",
      "Train loss 1.2782 | Test loss 2.6529 | Test acc 0.0\n",
      "Epoch 8\n",
      "-----\n",
      "Looked at 0/61664 samples\n",
      "Looked at 51200/61664 samples\n",
      "Train loss 1.2789 | Test loss 2.6559 | Test acc 0.0\n",
      "Epoch 9\n",
      "-----\n",
      "Looked at 0/61664 samples\n",
      "Looked at 51200/61664 samples\n",
      "Train loss 1.3562 | Test loss 2.6817 | Test acc 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "import numpy as np \n",
    "\n",
    "#Expecting the data to have around 160k to 240k total data samples for this feature\n",
    "\n",
    "proportion = int(0.8*len(input_features))\n",
    "x_train, y_train = input_features[:proportion], targeted_variable[:proportion]\n",
    "x_test, y_test = input_features[proportion:], targeted_variable[proportion:]\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "\n",
    "x_train, y_train = torch.from_numpy(x_train).type(torch.float32), torch.from_numpy(y_train).type(torch.float32)\n",
    "x_test, y_test = torch.from_numpy(x_test).type(torch.float32), torch.from_numpy(y_test).type(torch.float32)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                             shuffle=False, \n",
    "                             batch_size=BATCH_SIZE)\n",
    "#Create the neural network \n",
    "class ColorCode(nn.Module): \n",
    "    def __init__(self, \n",
    "                 hidden_size, \n",
    "                 num_layers, \n",
    "                 embedded_size,\n",
    "                 vocabulary_size, \n",
    "                 dropout_rate,  \n",
    "                 output_size): \n",
    "        super(ColorCode, self).__init__() \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedded_size = embedded_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedded_size)\n",
    "        self.lstm = nn.LSTM(input_size=embedded_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.stacked_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=1026), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout_rate), \n",
    "            nn.Linear(in_features=1026, out_features=526), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout_rate), \n",
    "            nn.Linear(in_features=526, out_features=256), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(in_features=256, out_features=126), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout_rate), \n",
    "            nn.Linear(in_features=126, out_features=64), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout_rate), \n",
    "            nn.Linear(in_features=64, out_features=32), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout_rate), \n",
    "            nn.Linear(in_features=32, out_features=output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        batch_size = x.size(0)\n",
    "        x = x.type(torch.LongTensor)\n",
    "        embedded_out = self.embedding(x)\n",
    "        #Output size of the embedded layer => (batch_size, sequence_length, output_dim)\n",
    "        embedded_out = embedded_out.transpose(1, 2)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        lstm_out, _ = self.lstm(embedded_out, (c0, h0))\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.stacked_layer(lstm_out)\n",
    "        return output \n",
    "#Initailize the model \n",
    "model = ColorCode(num_layers=1, hidden_size=16, vocabulary_size=vocabulary_size+1, embedded_size=4, output_size=4, dropout_rate=0.4)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "def acc_fn(y_pred, y_true): \n",
    "    correct = torch.eq(y_pred, y_true).sum().item() \n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc \n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Create the training and testing loop\n",
    "epoches = 10\n",
    "for epoch in range(epoches):\n",
    "    print(f'Epoch {epoch}\\n-----') \n",
    "    model.train() \n",
    "    for batch, (train_features, train_labels) in enumerate(train_dataloader):\n",
    "        train_pred = model(train_features)\n",
    "        train_loss = loss_fn(train_pred, train_labels.type(torch.long))\n",
    "        train_loss.backward() \n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "        if batch % 400 == 0: \n",
    "            print(f'Looked at {batch * len(train_features)}/{len(train_dataloader.dataset)} samples')\n",
    "        \n",
    "    with torch.inference_mode(): \n",
    "        model.eval() \n",
    "        for test_features, test_labels in test_dataloader: \n",
    "            test_pred = model(test_features)\n",
    "            test_loss = loss_fn(test_pred, test_labels.type(torch.long))\n",
    "            test_acc = acc_fn(torch.argmax(torch.softmax(test_pred, dim=1), dim=1), test_labels)\n",
    "        print(f'Train loss {train_loss.item():.4f} | Test loss {test_loss.item():.4f} | Test acc {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62891"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_column = machine_learning_df.columns.tolist()\n",
    "machine_learning_df.drop_duplicates(subset=machine_column, keep='last', ignore_index=True, inplace=True)\n",
    "len(machine_learning_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "continue_df6 = pd.read_excel('continue_working6.xlsx', names=['image_name', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from color_normalizing import ColorCode, SubCode \n",
    "folder_name = 'continue_working6.xlsx'\n",
    "sub_code_output = SubCode.dataframe_converting(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'color_normalizing' from 'd:\\\\Machine Learning\\\\Day 5 - LSTM\\\\color_normalizing.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib \n",
    "import color_normalizing\n",
    "importlib.reload(color_normalizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rank\n",
       "0    19292\n",
       "1    19270\n",
       "3    19264\n",
       "2    19254\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_learning_df['rank'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
